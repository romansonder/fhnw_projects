{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XAI Fachvertiefung\n",
        "\n",
        "Lecturer: Susanne Suter (susanne.suter@fhnw.ch)\n",
        "\n",
        "**Bemerkung**: __[Dies ist eine modifizierte Kopie eines iNNvestigate Github Repository Notebooks](https://github.com/albermax/innvestigate/blob/master/examples/imagenet_compare_methods.ipynb)__ (2023-01-06)\n"
      ],
      "metadata": {
        "id": "s9cl_-Cm7h8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Aufgaben Fachvertiefungsmodul: </font>\n",
        "\n",
        "* Führe dieses adaptierte Notebook von iNNvestigate durch. \n",
        "Bemerkung: Damit du schneller Daten visualisieren kannst, empfiehlt es sich, die Netze für weniger Epochen laufen zu lassen und nicht alle XAI Methoden von Innvestigate berechnen zu lassen (deshalb sind einige Methoden auskommentiert)\n",
        "\n",
        "* Beantworte folgende Fragen:\n",
        "    * Was bedeuten LRP-z bzw. LRP-Epsilon? Stellst du Unterschiede fest?\n",
        "    * Wofür ist die LRP-Methoden besonders gut geeignet? Im Vergleich zu den anderen Methoden?\n",
        "* Lade eigene Beispielbilder in den entsprechenden Ordner und versuche Beispiele zu finden, bei welchen die Relevanz zur Vorhersage nicht den Erwartungen entspricht\n",
        "* Vergleiche weiter von iNNvestigate implementierte XAI Methoden (Analysis Teil)\n",
        "* Spiele mit weiteren Notebooks von __[iNNvestigate](https://github.com/albermax/innvestigate/tree/master/examples)__"
      ],
      "metadata": {
        "id": "C7Ybd3SU6pWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layerwise Relevance Propagation (LRP)\n",
        "\n",
        "\n",
        "LRP ist eine Methode, die für die Entscheidung relevante Pixel identifiziert. Dafür wird in einem Rückwärtsdurchlauf durchs neuronale Netz konservatives Relevanzumverteilungsverfahren eingesetzt, bei welchem die Neuronen, die am meisten zur vorangehenden Schicht beitragen, die meiste Relevanz von dieser erhalten. \n",
        "\n",
        "\n",
        "Links: \n",
        "* __[Konferenz-Tutorial zu LRP](http://www.interpretable-ml.org/ecml2020tutorial/ecml2020-xai-tutorial-2.pdf)__ (Folien 29-34)\n",
        "* __[Chapter 10: Layer-Wise Relevance Propagation: An Overview](https://iphome.hhi.de/samek/pdf/MonXAI19.pdf)__\n",
        "* __[Heatmapping.org Webseite](http://www.heatmapping.org/)__ (gute Quelle für viele Tutorials und verlinkte Repositories)\n",
        "* __[Github Repository LRP](https://github.com/albermax/innvestigate)__ (TensorFlow - für PyTorch gibt es andere Bibliotheken; enthält diverse Beispiel-Notebooks)\n",
        "* __[Git Repository mit LRP Tutorial](https://git.tu-berlin.de/gmontavon/lrp-tutorial)__ (inkl. Publikationshinweise und Implementierungsdetails)"
      ],
      "metadata": {
        "id": "l3LH2kT16e_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before You Start"
      ],
      "metadata": {
        "id": "nQQqVYIc8gw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Click on \"Copy to Drive\" to work on your personal copy of the notebook. A Google account is needed for that.\n",
        "\n",
        "2) Select \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" : \"GPU\""
      ],
      "metadata": {
        "id": "jFuQUC1K8WPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Upload and Imports\n",
        "\n",
        "You need to upload the __[utils](https://github.com/albermax/innvestigate/tree/master/examples/utils)__ and __[images](https://github.com/albermax/innvestigate/tree/master/examples/images)__ from the iNNvestigate repository. This can either be done via Google drive (persistent) or every time you start a Colab session via manual upload of files (see left tab at bottom)."
      ],
      "metadata": {
        "id": "12MRoHZQqtja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive_dir = os.path.abspath('drive')\n",
        "drive.mount(drive_dir)"
      ],
      "metadata": {
        "id": "dIXlRMm5rA0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "b06Hut_Eq4ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downgrade the following libraries to be able to install iNNvestigate\n",
        "!pip install numpy==1.22.0"
      ],
      "metadata": {
        "id": "1UrSvNpp82e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Führe folgende Zeile aus, sofern du iNNvestigate noch nicht installiert hast. \n",
        "!pip install innvestigate"
      ],
      "metadata": {
        "id": "Wv3fK4_t8lpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDz4TBCm7TM0"
      },
      "source": [
        "# Compare analyzers on ImageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR0Ly_857TM2"
      },
      "source": [
        "In this notebook we show how one can use **iNNvestigate** to analyze the prediction of ImageNet-models! To do so we will load a network from the keras.applications module and analyze prediction on some images!\n",
        "\n",
        "Parts of the code that do not contribute to the main focus are outsourced into utility modules. To learn more about the basic usage of **iNNvestigate** have look into this notebook: [Introduction to iNNvestigate](introduction.ipynb) and [Comparing methods on MNIST](mnist_method_comparison.ipynb)\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NldmJ9vw7TM3"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Bh5GIR7TM3"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "# warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo0poX_v7TM4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import imp\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import innvestigate\n",
        "import innvestigate.utils as iutils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use utility libraries to focus on relevant iNNvestigate routines\n",
        "utils_dir = os.path.join(drive_dir, 'MyDrive', 'xai_data', 'utils')\n",
        "\n",
        "eutils = imp.load_source(\"utils\", os.path.join(utils_dir, \"__init__.py\"))\n",
        "imagenetutils = imp.load_source(\"utils_imagenet\", os.path.join(utils_dir, \"imagenet.py\"))"
      ],
      "metadata": {
        "id": "AR0UeEZfsFFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMhkH7VE7TM4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFX40LgG7TM4"
      },
      "source": [
        "In this demo use the VGG16-model, which uses ReLU activation layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tYF-O4eb7TM4"
      },
      "outputs": [],
      "source": [
        "# Load the model definition.\n",
        "model = VGG16(weights=\"imagenet\")\n",
        "\n",
        "# Handle input depending on model and backend.\n",
        "channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
        "color_conversion = \"BGRtoRGB\"  # keras.applications use BGR format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPwIKym7TM5"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut6nS_z-7TM5"
      },
      "source": [
        "Now we load some example images and preprocess them to fit the input size model.\n",
        "\n",
        "To analyze your own example images, just add them to `innvestigate/examples/images`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy-9yuKk7TM5"
      },
      "outputs": [],
      "source": [
        "# Get some example test set images.\n",
        "image_shape = [224, 224]\n",
        "images, label_to_class_name = eutils.get_imagenet_data(size=image_shape[0])\n",
        "\n",
        "if not len(images):\n",
        "    raise Exception(\n",
        "        \"Please download the example images using: \"\n",
        "        \"'innvestigate/examples/images/wget_imagenet_2011_samples.sh'\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqvvuaR-7TM5"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suzLTRkD7TM6"
      },
      "source": [
        "Next, we will set up a list of analysis methods by preparing tuples containing the methods' string identifiers used by `innvestigate.analyzer.create_analyzer(...)`, some optional parameters, a post processing choice for visualizing the computed analysis and a title for the figure to render. Analyzers can be deactivated by simply commenting the corresponding lines, or added by creating a new tuple as below.\n",
        "\n",
        "For a full list of methods refer to the dictionary `investigate.analyzer.analyzers`.\n",
        "\n",
        "Note: Should you run into resource trouble, e.g. you are running that notebook on a computer without GPU or with only limited GPU memory, consider deactivating one or more analyzers by commenting the corresponding lines in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WpEV8Qx7TM6"
      },
      "outputs": [],
      "source": [
        "input_range = (-128, 128)  # format used by keras.applications\n",
        "noise_scale = (input_range[1] - input_range[0]) * 0.1\n",
        "\n",
        "# Methods we use and some properties.\n",
        "\n",
        "\n",
        "methods = [\n",
        "    # NAME                  OPT.PARAMS                  POSTPROC FXN            TITLE\n",
        "    # Show input.\n",
        "    (\"input\",               {},                         imagenetutils.image,    \"Input\"),\n",
        "    # Function\n",
        "    (\"gradient\",            {\"postprocess\": \"abs\"},     imagenetutils.graymap,  \"Gradient\"),\n",
        "    #(\"smoothgrad\",          {\"augment_by_n\": 64, \n",
        "    #                         \"noise_scale\": noise_scale, \n",
        "    #                         \"postprocess\": \"square\"},  imagenetutils.graymap,  \"SmoothGrad\"),\n",
        "    # Signal\n",
        "    #(\"deconvnet\",           {},                         imagenetutils.bk_proj,  \"Deconvnet\"),\n",
        "    #(\"guided_backprop\",     {},                         imagenetutils.bk_proj,  \"Guided Backprop\"),\n",
        "    # Interaction\n",
        "    #(\"deep_taylor.bounded\", {\"low\": input_range[0], \n",
        "    #                         \"high\": input_range[1]},   imagenetutils.heatmap,  \"DeepTaylor\"),\n",
        "    #(\"input_t_gradient\",    {},                         imagenetutils.heatmap,  \"Input * Gradient\"),\n",
        "    #(\"integrated_gradients\",\n",
        "    #                        {\"reference_inputs\": input_range[0], \n",
        "    #                         \"steps\": 64},              imagenetutils.heatmap,  \"Integrated Gradients\"),\n",
        "    (\"lrp.z\",               {},                         imagenetutils.heatmap,  \"LRP-Z\"),\n",
        "    (\"lrp.epsilon\",         {\"epsilon\": 1},             imagenetutils.heatmap,  \"LRP-Epsilon\"),\n",
        "    (\"lrp.sequential_preset_a_flat\",\n",
        "                            {\"epsilon\": 1},             imagenetutils.heatmap,  \"LRP-PresetAFlat\"),\n",
        "    (\"lrp.sequential_preset_b_flat\",\n",
        "                            {\"epsilon\": 1},             imagenetutils.heatmap,  \"LRP-PresetBFlat\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUfnQFdv7TM6"
      },
      "source": [
        "The main loop below will now instantiate the analyzer objects based on the loaded/trained model and the analyzers' parameterizations above and compute the analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWgcTE4w7TM6"
      },
      "outputs": [],
      "source": [
        "# Create model without trailing softmax\n",
        "model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
        "\n",
        "# Create analyzers.\n",
        "analyzers = []\n",
        "for method in methods:\n",
        "    try:\n",
        "        analyzer = innvestigate.create_analyzer(\n",
        "            method[0],  # analysis method identifier\n",
        "            model_wo_softmax,  # model without softmax output\n",
        "            **method[1]\n",
        "        )  # optional analysis parameters\n",
        "    except innvestigate.NotAnalyzeableModelException:\n",
        "        # Not all methods work with all models.\n",
        "        analyzer = None\n",
        "    analyzers.append(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVa13cV17TM7"
      },
      "source": [
        "Now we analyze each image with the different analyzers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQmSDke37TM7"
      },
      "outputs": [],
      "source": [
        "analysis = np.zeros([len(images), len(analyzers)] + image_shape + [3])\n",
        "text = []\n",
        "\n",
        "\n",
        "for i, (x, y) in enumerate(images):\n",
        "    # Add batch axis.\n",
        "    x = x[None, :, :, :]\n",
        "    x_pp = preprocess_input(x)\n",
        "\n",
        "    # Predict final activations, probabilites, and label.\n",
        "    presm = model_wo_softmax.predict_on_batch(x_pp)[0]\n",
        "    prob = model.predict_on_batch(x_pp)[0]\n",
        "    y_hat = prob.argmax()\n",
        "\n",
        "    # Save prediction info:\n",
        "    text.append(\n",
        "        (\n",
        "            \"%s\" % label_to_class_name[y],  # ground truth label\n",
        "            \"%.2f\" % presm.max(),  # pre-softmax logits\n",
        "            \"%.2f\" % prob.max(),  # probabilistic softmax output\n",
        "            \"%s\" % label_to_class_name[y_hat],  # predicted label\n",
        "        )\n",
        "    )\n",
        "\n",
        "    for aidx, analyzer in enumerate(analyzers):\n",
        "        if methods[aidx][0] == \"input\":\n",
        "            # Do not analyze, but keep not preprocessed input.\n",
        "            a = x / 255\n",
        "        elif analyzer:\n",
        "            # Analyze.\n",
        "            a = analyzer.analyze(x_pp)\n",
        "\n",
        "            # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
        "            a = imagenetutils.postprocess(a, color_conversion, channels_first)\n",
        "            # Apply analysis postprocessing, e.g., creating a heatmap.\n",
        "            a = methods[aidx][2](a)\n",
        "        else:\n",
        "            a = np.zeros_like(image)\n",
        "        # Store the analysis.\n",
        "        analysis[i, aidx] = a[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzrKiNiP7TM7"
      },
      "source": [
        "Next, we visualize the analysis results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "0ot-VOyP7TM8"
      },
      "outputs": [],
      "source": [
        "# Prepare the grid as rectangular list\n",
        "grid = [\n",
        "    [analysis[i, j] for j in range(analysis.shape[1])] for i in range(analysis.shape[0])\n",
        "]\n",
        "# Prepare the labels\n",
        "label, presm, prob, pred = zip(*text)\n",
        "row_labels_left = [\n",
        "    (\"label: {}\".format(label[i]), \"pred: {}\".format(pred[i]))\n",
        "    for i in range(len(label))\n",
        "]\n",
        "row_labels_right = [\n",
        "    (\"logit: {}\".format(presm[i]), \"prob: {}\".format(prob[i]))\n",
        "    for i in range(len(label))\n",
        "]\n",
        "col_labels = [\"\".join(method[3]) for method in methods]\n",
        "\n",
        "# Plot the analysis.\n",
        "eutils.plot_image_grid(\n",
        "    grid,\n",
        "    row_labels_left,\n",
        "    row_labels_right,\n",
        "    col_labels,\n",
        "    file_name=os.environ.get(\"plot_file_name\", None),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "i1AlNzuQ7TM8"
      },
      "source": [
        "This figure shows the analysis regarding the *actually predicted* class as computed by the selected analyzers. Each column shows the visualized results for different analyzers and each row shows the analyses wrt to one input sample. To the left of each row, the ground truth label `label` and the predicted label `pred` are show. To the right, the model's probabilistic (softmax) output is shown as `prob` and the logit output just before the terminating softmax layer as `logit`. Note that all analyses have been performed based on the logit output (layer).\n",
        "\n",
        "\n",
        "If you are curious about how **iNNvestigate** performs on *different* ImageNet model, have a look here: [Comparing networks on ImageNet](imagenet_network_comparison.ipynb)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "3d32a2c4e0654b97e3d0f99d8973ef47a5cc5e9b1c715949292df3e3083fd539"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('innvestigate-pdNhrmV2-py3.7': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}